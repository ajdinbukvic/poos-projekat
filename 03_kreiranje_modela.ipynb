{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library to use array\n",
    "import pandas as pd # used for datasets \n",
    "import matplotlib.pyplot as plt  # plotting library for creating visualizations in Python.\n",
    "import cv2  # used for computer vision and image processing tasks.\n",
    "import tensorflow as tf  # open-source machine learning framework developed by Google.\n",
    "from PIL import Image  # provides support for opening, manipulating, and saving many different image file formats.\n",
    "import os  # provides a way of using operating system-dependent functionality.\n",
    "from sklearn.model_selection import train_test_split  # Used for splitting datasets into training and testing sets.\n",
    "from keras.utils import to_categorical  # A high-level neural networks API. to_categorical is used for one-hot encoding categorical variables.\n",
    "from keras.models import Sequential, load_model  # A linear stack of layers. load_model is used to load pre-trained Keras models.\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout  # Layers used to build convolutional neural networks (CNNs) in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from PIL import Image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../input/gtsrb-german-traffic-sign'\n",
    "train_path = '../input/gtsrb-german-traffic-sign/Train'\n",
    "test_path = '../input/gtsrb-german-traffic-sign/'\n",
    "IMG_HEIGHT = 30\n",
    "IMG_WIDTH = 30\n",
    "channels = 3\n",
    "NUM_CATEGORIES = len(os.listdir(train_path))\n",
    "NUM_CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "   \n",
    "    images = list()\n",
    "    labels = list()\n",
    "    for category in range(NUM_CATEGORIES):\n",
    "        categories = os.path.join(data_dir, str(category))\n",
    "        for img in os.listdir(categories):\n",
    "            img = load_img(os.path.join(categories, img), target_size=(30, 30))\n",
    "            image = img_to_array(img)\n",
    "            images.append(image)\n",
    "            labels.append(category)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_data(train_path)\n",
    "images = np.array(images)\n",
    "images = images/255\n",
    "classNo = np.array(labels)\n",
    "data=np.array(images)\n",
    "data= np.array(data).reshape(-1, 32, 32, 3)\n",
    "\n",
    "# One hot encoding the labels\n",
    "# labels = to_categorical(labels)\n",
    "\n",
    "X = images.astype(np.float32)\n",
    "y = labels.astype(np.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "Y_tests=y_test\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2) \n",
    "\n",
    "print(\"Data Shapes\")\n",
    "print(\"Train\",end = \"\");print(X_train.shape,y_train.shape)\n",
    "print(\"Validation\",end = \"\");print(X_validation.shape,y_validation.shape)\n",
    "print(\"Test\",end = \"\");print(X_test.shape,y_test.shape)\n",
    "\n",
    "# Splitting the dataset into training and test set\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#                                                     np.array(images),\n",
    "#                                                     labels,\n",
    "#                                                     test_size=0.2 , \n",
    "#                                                     random_state=42, \n",
    "#                                                     shuffle=True\n",
    "#                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train.shape\", x_train.shape)\n",
    "print(\"X_valid.shape\", x_test.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"y_valid.shape\", y_test.shape)\n",
    "X_train.shape (31367, 30, 30, 3)\n",
    "X_valid.shape (7842, 30, 30, 3)\n",
    "y_train.shape (31367, 43)\n",
    "y_valid.shape (7842, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "\n",
    "\n",
    "def equalize(img):\n",
    "    img = cv2.equalizeHist(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocessing(img):\n",
    "    img = grayscale(img) \n",
    "    img = equalize(img)  \n",
    "    img = img / 255 # image normalization \n",
    "    return img\n",
    "\n",
    "\n",
    "X_train = np.array(list(map(preprocessing, X_train)))\n",
    "X_validation = np.array(list(map(preprocessing, X_validation)))\n",
    "X_test = np.array(list(map(preprocessing, X_test)))\n",
    "### reshape data into channel 1\n",
    "X_train=X_train.reshape(-1,32,32,1)\n",
    "X_validation=X_validation.reshape(-1,32,32,1)\n",
    "X_test=X_test.reshape(-1,32,32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             zoom_range=0.2,\n",
    "                             shear_range=0.1,\n",
    "                             rotation_range=10)\n",
    "dataGen.fit(X_train)\n",
    "batches = dataGen.flow(X_train, y_train,batch_size=20)\n",
    "X_batch, y_batch = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, noOfClasses)\n",
    "y_validation = to_categorical(y_validation, noOfClasses)\n",
    "y_test = to_categorical(y_test, noOfClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), activation=\"relu\", input_shape=(IMG_HEIGHT,IMG_WIDTH,3)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(43, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_path = \"../input/gtsrb-german-traffic-sign/Train\"\n",
    "data_list = []\n",
    "labels_list = []\n",
    "classes_list = 43\n",
    "for i in range(classes_list):\n",
    "    i_path = os.path.join(imgs_path, str(i)) #0-42\n",
    "    for img in os.listdir(i_path):\n",
    "        im = Image.open(i_path +'/'+ img)\n",
    "        im = im.resize((30,30))\n",
    "        im = np.array(im)\n",
    "        data_list.append(im)\n",
    "        labels_list.append(i)\n",
    "data = np.array(data_list)\n",
    "labels = np.array(labels_list)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(X,y):\n",
    "    X_prep = X.astype('float32')\n",
    "    y_prep = to_categorical(np.array(y))\n",
    "    return (X_prep, y_prep)\n",
    "\n",
    "X, y = prep_dataset(data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X,y, test_size=0.2, shuffle=True,stratify=y)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val,Y_val, test_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "EPOCHS = 30\n",
    "history = model.fit(aug.flow(x_train, \n",
    "                    y_train, batch_size = 32),\n",
    "                    validation_data = (x_test, y_test), \n",
    "                    epochs=EPOCHS   \n",
    "                   )\n",
    "\n",
    "# history= model.fit(X_train,Y_train,\n",
    "#                  epochs=15,\n",
    "#                  batch_size=64,\n",
    "#                  validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig,ax=plt.subplots(2,1,figsize=(12,10))\n",
    "fig.suptitle('Train evaluation')\n",
    "\n",
    "sns.lineplot(ax= ax[0],x=np.arange(0,len(history.history['accuracy'])),y=history.history['accuracy'])\n",
    "sns.lineplot(ax= ax[0],x=np.arange(0,len(history.history['accuracy'])),y=history.history['val_accuracy'])\n",
    "\n",
    "ax[0].legend(['Train','Validation'])\n",
    "ax[0].set_title('Accuracy')\n",
    "\n",
    "sns.lineplot(ax= ax[1],x=np.arange(0,len(history.history['loss'])),y=history.history['loss'])\n",
    "sns.lineplot(ax= ax[1],x=np.arange(0,len(history.history['loss'])),y=history.history['val_loss'])\n",
    "\n",
    "ax[1].legend(['Train','Validation'])\n",
    "ax[1].set_title('Loss')\n",
    "print(fig)\n",
    "print(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('test set accuracy: ', accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "Y_test = pd.read_csv(test_path + 'Test.csv')\n",
    "test_labels = Y_test[\"ClassId\"].values\n",
    "test_images = Y_test[\"Path\"].values\n",
    "\n",
    "output = list()\n",
    "for img in test_images:\n",
    "    image = load_img(os.path.join(test_path, img), target_size=(30, 30))\n",
    "    output.append(np.array(image))\n",
    "\n",
    "X_test=np.array(output)\n",
    "pred = model.predict(X_test)\n",
    "pred=np.argmax(pred, axis=1)\n",
    "\n",
    "#Accuracy with the test data\n",
    "print('Test Data accuracy: ',accuracy_score(test_labels, pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 16))\n",
    "\n",
    "start_index = 0\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    prediction = pred[start_index + i]\n",
    "    actual = test_labels[start_index + i]\n",
    "    col = 'g'\n",
    "    if prediction != actual:\n",
    "        col = 'r'\n",
    "    plt.xlabel('Actual={} || Pred={}'.format(actual, prediction), color = col)\n",
    "    plt.imshow(X_test[start_index + i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(test_labels,pred )\n",
    "import seaborn as sns\n",
    "df_cm = pd.DataFrame(cf, index = classes,  columns = classes)\n",
    "plt.figure(figsize = (20,20))\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report for the Validation Dataset\n",
    "val_prob = model.predict(x_val)\n",
    "#convert tests labels in single-digits instead of one-hot encoding\n",
    "y_val_arg = np.argmax(y_val,axis=1)\n",
    "val_predicted_labels = np.argmax(val_prob, axis = 1) #take argmax because the class with the highest probability would be the predicted class\n",
    "val_report = classification_report(y_val_arg,val_predicted_labels)\n",
    "print('---')\n",
    "print('Classification report for Validation Dataset:')\n",
    "print(val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "data = []  # List to store the images.\n",
    "labels = []  # List to store the corresponding labels.\n",
    "classes = 43  # Total number of classes/categories in the dataset.\n",
    "cur_path = os.getcwd()  # Get the current working directory.\n",
    "\n",
    "# Loop through each directory in the current path, representing different classes.\n",
    "for i in os.listdir(cur_path):\n",
    "    dir = cur_path + '/' + i  # Create the path for the current class directory.\n",
    "    \n",
    "    # Loop through each file (image) in the current class directory.\n",
    "    for j in os.listdir(dir):\n",
    "        img_path = dir+'/'+j  # Create the full path for the current image.\n",
    "        \n",
    "        # Read the image using OpenCV, resize it to (30, 30), and store it in the data list.\n",
    "        img = cv2.imread(img_path, -1)\n",
    "        img = cv2.resize(img, (30, 30), interpolation=cv2.INTER_NEAREST)\n",
    "        data.append(img)\n",
    "        labels.append(i) # Append the corresponding label (class) to the labels list.\n",
    "\n",
    "# Convert data and labels lists to NumPy arrays for further processing.\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Print the shape of the data (images) and labels arrays.\n",
    "print(data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting training and testing dataset\n",
    "# Using train_test_split to divide the dataset into training and testing sets.\n",
    "# The test_size parameter determines the proportion of the dataset to include in the test split.\n",
    "# random_state is set for reproducibility of results.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and testing sets to verify the split.\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the labels into one hot encoding\n",
    "y_train = to_categorical(y_train, 43)\n",
    "y_test = to_categorical(y_test, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape # Print the shape of the training labels (y_train) and (y_test) array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the model\n",
    "model = Sequential()\n",
    "\n",
    "# First Layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "\n",
    "# Second Layer \n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "\n",
    "\n",
    "# Dense Layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(43, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation of the model\n",
    "# Configuring the model for training with categorical crossentropy loss, Adam optimizer, and accuracy metric.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs.\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model on the training data (X_train, y_train) with batch size 64,\n",
    "# and validating on the testing data (X_test, y_test).\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained model to a file named 'my_model.h5' in the working directory.\n",
    "model.save('./modeli/model_1.h5')\n",
    "model.save('./modeli/model_1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting graphs for accuracy\n",
    "# Plotting the training accuracy over epochs.\n",
    "plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "\n",
    "# Plotting the validation accuracy over epochs.\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "\n",
    "# Adding a title to the plot.\n",
    "plt.title('Accuracy')\n",
    "\n",
    "# Labeling the x-axis as 'epochs'.\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "# Labeling the y-axis as 'accuracy'.\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "# Adding a legend to distinguish between training and validation accuracy.\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting graphs for loss\n",
    "# Plotting the training loss over epochs.\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "\n",
    "# Plotting the validation loss over epochs.\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "\n",
    "# Adding a title to the plot.\n",
    "plt.title('Loss')\n",
    "\n",
    "# Labeling the x-axis as 'epochs'.\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "# Labeling the y-axis as 'loss'.\n",
    "plt.ylabel('loss')\n",
    "\n",
    "# Adding a legend to distinguish between training and validation loss.\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "# Evaluate the model on the testing data and store the results in the variable 'score'.\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the test loss (index 0 of the score array).\n",
    "print('Test Loss', score[0])\n",
    "\n",
    "# Print the test accuracy (index 1 of the score array).\n",
    "print('Test accuracy', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the testing data using the trained model.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Extract the true class labels from the one-hot encoded 'y_test'.\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Extract the predicted class labels from the one-hot encoded 'y_pred'.\n",
    "y_pred_class = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions from scikit-learn for generating classification reports and confusion matrices.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Print the classification report, which includes precision, recall, and F1-score.\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "\n",
    "# Print the confusion matrix, which shows the number of true positive, true negative, false positive, and false negative predictions.\n",
    "print(confusion_matrix(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "# Import the confusion_matrix function from scikit-learn.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute the confusion matrix based on true class labels (y_test_class) and predicted class labels (y_pred_class).\n",
    "cm = confusion_matrix(y_test_class, y_pred_class)\n",
    "\n",
    "# Import the seaborn library for data visualization.\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a heatmap of the confusion matrix with annotations.\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "# Save the heatmap as an image file named 'h1.png' in the working directory.\n",
    "plt.savefig('/kaggle/working/h1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Accuracy\n",
    "# Import the accuracy_score function from scikit-learn.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the accuracy by comparing predicted class labels (y_pred_class) with true class labels (y_test_class).\n",
    "score = accuracy_score(y_pred_class, y_test_class)\n",
    "\n",
    "# Display the computed accuracy score.\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
